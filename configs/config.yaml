# --- PyTorch Lightning Trainer Configuration ---
# See: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-class-api
trainer:
  default_root_dir: "./output_pl"  # Directory for logs and checkpoints
  accelerator: "gpu"
  devices: 3                      # Number of GPUs to use
  strategy: "fsdp_native"         # Fully Sharded Data Parallelism for model parallelism
  precision: "bf16-mixed"         # "bf16-mixed" for Ampere/Ada GPUs, "16-mixed" for others
  max_epochs: 100
  log_every_n_steps: 10
  accumulate_grad_batches: 4      # Effective batch size = batch_size * devices * accumulate_grad_batches

# --- Data Configuration (for CTDataModule) ---
data:
  base_dir: "/path/to/xray/data"
  base_dir2: "/path/to/ct/data"
  batch_size: 1                   # Per-GPU batch size. MUST be 1 for large images.
  num_workers: 4
  train_val_split_ratio: 0.9

# --- Training Configuration ---
training:
  seed: 42

# --- Model Configuration (for DiffusionLitModule) ---
model:
  # Flag to enable gradient checkpointing in the UNet
  gradient_checkpointing: true    # Crucial for saving memory

  # Configuration for the X-ray encoder
  encoder_config:
    out_dim: 768
    sequence_length: 16

  # Configuration for the 3D U-Net (diffusers.UNet3DConditionModel)
  # For (256, 256, 256) images, memory is a major concern. Start with a smaller model.
  unet_config:
    sample_size: 256
    in_channels: 1
    out_channels: 1
    block_out_channels: [32, 64, 128, 256] # Controls model size.
    down_block_types:
      - "DownBlock3D"
      - "DownBlock3D"
      - "AttnDownBlock3D"
      - "AttnDownBlock3D"
    up_block_types:
      - "AttnUpBlock3D"
      - "AttnUpBlock3D"
      - "UpBlock3D"
      - "UpBlock3D"

# --- Scheduler Configuration (for DDPMScheduler) ---
scheduler:
  num_train_timesteps: 1000
  beta_schedule: "squaredcos_cap_v2"

# --- Optimizer Configuration (for AdamW) ---
optimizer:
  learning_rate: 1.0e-4
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_weight_decay: 1.0e-2
  adam_epsilon: 1.0e-8